{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1feaae0",
   "metadata": {},
   "source": [
    "# ğŸ“Š Pandas Tutorial 1: Reading and Inspecting Data\n",
    "\n",
    "Welcome to the first notebook in our comprehensive Pandas learning series! This notebook covers the fundamentals of reading data from various sources and performing initial data exploration.\n",
    "\n",
    "## ğŸ¯ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "- Load data from CSV files using pandas\n",
    "- Understand different data types in pandas\n",
    "- Explore data structure and dimensions\n",
    "- Generate basic statistical summaries\n",
    "- Identify missing values and data quality issues\n",
    "\n",
    "## ğŸ“ Dataset Overview\n",
    "\n",
    "We'll work with three sample datasets:\n",
    "1. **Sales Data** - Product sales information with dates, categories, and revenue\n",
    "2. **Employee Data** - HR data with salaries, departments, and performance scores\n",
    "3. **Weather Data** - Temperature and weather conditions across different cities\n",
    "\n",
    "Let's get started! ğŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e10dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set display options for better output\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "print(\"ğŸ“š Libraries imported successfully!\")\n",
    "print(f\"ğŸ¼ Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c74a6d",
   "metadata": {},
   "source": [
    "## ğŸ“– Section 1: Loading Data from CSV Files\n",
    "\n",
    "The most common way to load data into pandas is from CSV files. Let's load our three sample datasets and explore them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e1e8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the three sample datasets\n",
    "# Note: Adjust paths if running from different directory\n",
    "\n",
    "# 1. Sales Data\n",
    "sales_df = pd.read_csv('../data/sales_data.csv')\n",
    "print(\"âœ… Sales data loaded successfully!\")\n",
    "\n",
    "# 2. Employee Data  \n",
    "employees_df = pd.read_csv('../data/employees.csv')\n",
    "print(\"âœ… Employee data loaded successfully!\")\n",
    "\n",
    "# 3. Weather Data\n",
    "weather_df = pd.read_csv('../data/weather_data.csv')\n",
    "print(\"âœ… Weather data loaded successfully!\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Loaded {len(sales_df)} sales records\")\n",
    "print(f\"ğŸ‘¥ Loaded {len(employees_df)} employee records\") \n",
    "print(f\"ğŸŒ¤ï¸ Loaded {len(weather_df)} weather records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29da81be",
   "metadata": {},
   "source": [
    "## ğŸ” Section 2: First Look at the Data\n",
    "\n",
    "Let's examine the structure and first few rows of each dataset using various pandas methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fe2bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the Sales Data\n",
    "print(\"ğŸ›’ SALES DATA OVERVIEW\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Shape: {sales_df.shape}\")\n",
    "print(f\"Columns: {list(sales_df.columns)}\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "display(sales_df.head())\n",
    "\n",
    "print(\"\\nLast 3 rows:\")\n",
    "display(sales_df.tail(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0da6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the Employee Data\n",
    "print(\"ğŸ‘¥ EMPLOYEE DATA OVERVIEW\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Shape: {employees_df.shape}\")\n",
    "print(f\"Columns: {list(employees_df.columns)}\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "display(employees_df.head())\n",
    "\n",
    "print(\"\\nColumn data types:\")\n",
    "print(employees_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88baba53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the Weather Data\n",
    "print(\"ğŸŒ¤ï¸ WEATHER DATA OVERVIEW\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Shape: {weather_df.shape}\")\n",
    "print(f\"Columns: {list(weather_df.columns)}\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "display(weather_df.head())\n",
    "\n",
    "print(\"\\nSample of data:\")\n",
    "display(weather_df.sample(5))  # Random sample of 5 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d55f2c2",
   "metadata": {},
   "source": [
    "## ğŸ“Š Section 3: Data Information and Summary Statistics\n",
    "\n",
    "The `.info()` method provides a concise summary of your DataFrame, while `.describe()` gives statistical summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e071a202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get detailed information about each dataset\n",
    "print(\"ğŸ“‹ SALES DATA INFO\")\n",
    "print(\"=\" * 40)\n",
    "sales_df.info()\n",
    "\n",
    "print(\"\\nğŸ“‹ EMPLOYEE DATA INFO\")\n",
    "print(\"=\" * 40)\n",
    "employees_df.info()\n",
    "\n",
    "print(\"\\nğŸ“‹ WEATHER DATA INFO\")\n",
    "print(\"=\" * 40)\n",
    "weather_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272244bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate statistical summaries for numerical columns\n",
    "print(\"ğŸ“ˆ SALES DATA - NUMERICAL SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "display(sales_df.describe())\n",
    "\n",
    "print(\"\\nğŸ“ˆ EMPLOYEE DATA - NUMERICAL SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "display(employees_df.describe())\n",
    "\n",
    "print(\"\\nğŸ“ˆ WEATHER DATA - NUMERICAL SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "display(weather_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1370156b",
   "metadata": {},
   "source": [
    "## ğŸ¯ Section 4: Understanding Data Types\n",
    "\n",
    "Understanding data types is crucial for proper data analysis. Let's explore the different types in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadc95be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's convert the Date column to datetime for better analysis\n",
    "print(\"ğŸ“… CONVERTING DATE COLUMNS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Sales data date conversion\n",
    "print(\"Before conversion:\")\n",
    "print(f\"Sales Date column type: {sales_df['Date'].dtype}\")\n",
    "print(f\"Sample values: {sales_df['Date'].head(3).tolist()}\")\n",
    "\n",
    "# Convert to datetime\n",
    "sales_df['Date'] = pd.to_datetime(sales_df['Date'])\n",
    "print(f\"\\nAfter conversion:\")\n",
    "print(f\"Sales Date column type: {sales_df['Date'].dtype}\")\n",
    "print(f\"Sample values: {sales_df['Date'].head(3).tolist()}\")\n",
    "\n",
    "# Employee data date conversion\n",
    "employees_df['Join_Date'] = pd.to_datetime(employees_df['Join_Date'])\n",
    "print(f\"\\nEmployee Join_Date column type: {employees_df['Join_Date'].dtype}\")\n",
    "\n",
    "# Weather data date conversion  \n",
    "weather_df['Date'] = pd.to_datetime(weather_df['Date'])\n",
    "print(f\"Weather Date column type: {weather_df['Date'].dtype}\")\n",
    "\n",
    "print(\"\\nâœ… All date columns converted successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1011db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore unique values in categorical columns\n",
    "print(\"ğŸ·ï¸ EXPLORING CATEGORICAL DATA\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Sales data categories\n",
    "print(\"Sales - Product categories:\")\n",
    "print(sales_df['Product'].value_counts())\n",
    "print(f\"\\nSales - Unique categories: {sales_df['Category'].unique()}\")\n",
    "print(f\"Sales - Unique regions: {sales_df['Region'].unique()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"Employee - Department distribution:\")\n",
    "print(employees_df['Department'].value_counts())\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"Weather - Cities in dataset:\")\n",
    "print(weather_df['City'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1f63d9",
   "metadata": {},
   "source": [
    "## ğŸš¨ Section 5: Data Quality Assessment\n",
    "\n",
    "Let's check for missing values, duplicates, and other data quality issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e6513f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in all datasets\n",
    "print(\"ğŸ” MISSING VALUES ANALYSIS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "datasets = {\n",
    "    'Sales': sales_df,\n",
    "    'Employees': employees_df, \n",
    "    'Weather': weather_df\n",
    "}\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    print(f\"\\n{name} Dataset:\")\n",
    "    missing = df.isnull().sum()\n",
    "    missing_pct = (missing / len(df)) * 100\n",
    "    \n",
    "    if missing.sum() == 0:\n",
    "        print(\"  âœ… No missing values found!\")\n",
    "    else:\n",
    "        print(\"  Missing values:\")\n",
    "        for col, count in missing[missing > 0].items():\n",
    "            print(f\"    {col}: {count} ({missing_pct[col]:.1f}%)\")\n",
    "\n",
    "# Check for duplicate rows\n",
    "print(f\"\\nğŸ”„ DUPLICATE ROWS CHECK\")\n",
    "print(\"=\" * 40)\n",
    "for name, df in datasets.items():\n",
    "    duplicates = df.duplicated().sum()\n",
    "    print(f\"{name} Dataset: {duplicates} duplicate rows\")\n",
    "    if duplicates > 0:\n",
    "        print(f\"  â†’ {duplicates/len(df)*100:.1f}% of total rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60426f30",
   "metadata": {},
   "source": [
    "## ğŸ“ˆ Section 6: Quick Visual Exploration\n",
    "\n",
    "Let's create some simple visualizations to better understand our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f82e9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some basic visualizations\n",
    "plt.style.use('default')\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Sales by Product\n",
    "sales_df['Product'].value_counts().plot(kind='bar', ax=axes[0,0], color='skyblue')\n",
    "axes[0,0].set_title('ğŸ“Š Sales Count by Product', fontsize=12, fontweight='bold')\n",
    "axes[0,0].set_ylabel('Count')\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Revenue by Region\n",
    "region_revenue = sales_df.groupby('Region')['Revenue'].sum()\n",
    "region_revenue.plot(kind='pie', ax=axes[0,1], autopct='%1.1f%%', startangle=90)\n",
    "axes[0,1].set_title('ğŸ’° Revenue Distribution by Region', fontsize=12, fontweight='bold')\n",
    "axes[0,1].set_ylabel('')\n",
    "\n",
    "# Employee Age Distribution\n",
    "employees_df['Age'].hist(bins=10, ax=axes[1,0], color='lightgreen', alpha=0.7)\n",
    "axes[1,0].set_title('ğŸ‘¥ Employee Age Distribution', fontsize=12, fontweight='bold')\n",
    "axes[1,0].set_xlabel('Age')\n",
    "axes[1,0].set_ylabel('Frequency')\n",
    "\n",
    "# Temperature by City\n",
    "for city in weather_df['City'].unique():\n",
    "    city_data = weather_df[weather_df['City'] == city]\n",
    "    axes[1,1].plot(city_data['Date'], city_data['Temperature'], marker='o', label=city)\n",
    "axes[1,1].set_title('ğŸŒ¡ï¸ Temperature Trends by City', fontsize=12, fontweight='bold')\n",
    "axes[1,1].set_xlabel('Date')\n",
    "axes[1,1].set_ylabel('Temperature (Â°C)')\n",
    "axes[1,1].legend()\n",
    "axes[1,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b77a97",
   "metadata": {},
   "source": [
    "## ğŸ“ Section 7: Key Takeaways and Next Steps\n",
    "\n",
    "### What We've Learned:\n",
    "\n",
    "1. **ğŸ“ Data Loading**: How to read CSV files using `pd.read_csv()`\n",
    "2. **ğŸ” Data Exploration**: Using `.head()`, `.tail()`, `.info()`, and `.describe()` \n",
    "3. **ğŸ“Š Data Structure**: Understanding DataFrame shape, columns, and data types\n",
    "4. **ğŸ¯ Data Types**: Converting strings to datetime objects for better analysis\n",
    "5. **ğŸš¨ Data Quality**: Checking for missing values and duplicates\n",
    "6. **ğŸ“ˆ Quick Visualization**: Creating basic plots to understand data patterns\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "In the next notebook, we'll dive deeper into:\n",
    "- ğŸ” **Data Filtering and Selection** - Finding specific subsets of data\n",
    "- ğŸ”„ **Data Grouping and Aggregation** - Summarizing data by categories  \n",
    "- ğŸ§® **Statistical Analysis** - Calculating custom metrics and insights\n",
    "- ğŸ”— **Data Merging** - Combining multiple datasets\n",
    "\n",
    "### ğŸ’¡ Pro Tips:\n",
    "\n",
    "- Always inspect your data before analysis\n",
    "- Convert date columns to datetime for time-based operations\n",
    "- Check for missing values and outliers early\n",
    "- Use descriptive variable names and comments in your code"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
