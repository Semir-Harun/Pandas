{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2a8e8de",
   "metadata": {},
   "source": [
    "# üîé Pandas Tutorial 3: Data Filtering and Cleaning\n",
    "\n",
    "Welcome to the third notebook in our comprehensive Pandas series! This notebook covers essential data filtering, selection, and cleaning techniques.\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "- Filter data using boolean indexing and query methods\n",
    "- Select specific rows and columns efficiently\n",
    "- Handle missing data with various strategies\n",
    "- Clean and transform text data\n",
    "- Work with datetime data for time-based analysis\n",
    "- Remove duplicates and handle outliers\n",
    "\n",
    "## üßπ Why Data Cleaning Matters\n",
    "\n",
    "Real-world data is often messy! Data cleaning typically takes 80% of a data scientist's time. This notebook will give you the tools to:\n",
    "- üîç Find and filter relevant data subsets\n",
    "- üßº Clean messy text and categorical data\n",
    "- ‚è∞ Handle time series data effectively\n",
    "- üö® Deal with missing values and outliers\n",
    "\n",
    "Let's dive in! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe545d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and load data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Set pandas display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "# Load our sample datasets\n",
    "sales_df = pd.read_csv('../data/sales_data.csv')\n",
    "employees_df = pd.read_csv('../data/employees.csv')\n",
    "weather_df = pd.read_csv('../data/weather_data.csv')\n",
    "\n",
    "# Convert date columns\n",
    "sales_df['Date'] = pd.to_datetime(sales_df['Date'])\n",
    "employees_df['Join_Date'] = pd.to_datetime(employees_df['Join_Date'])\n",
    "weather_df['Date'] = pd.to_datetime(weather_df['Date'])\n",
    "\n",
    "print(\"‚úÖ Data loaded and preprocessed successfully!\")\n",
    "print(f\"üìä Sales: {sales_df.shape}, üë• Employees: {employees_df.shape}, üå§Ô∏è Weather: {weather_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea91ced",
   "metadata": {},
   "source": [
    "## üîç Section 1: Boolean Indexing and Filtering\n",
    "\n",
    "Boolean indexing is one of the most powerful features in pandas for filtering data based on conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc7e106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Boolean Indexing Examples\n",
    "print(\"üîç BOOLEAN INDEXING EXAMPLES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Simple condition filtering\n",
    "high_revenue = sales_df[sales_df['Revenue'] > 1000]\n",
    "print(f\"Sales with revenue > $1000: {len(high_revenue)} out of {len(sales_df)}\")\n",
    "display(high_revenue.head())\n",
    "\n",
    "# 2. Multiple conditions using & (and) and | (or)\n",
    "electronics_north = sales_df[(sales_df['Category'] == 'Electronics') & (sales_df['Region'] == 'North')]\n",
    "print(f\"\\nElectronics in North region: {len(electronics_north)} records\")\n",
    "display(electronics_north)\n",
    "\n",
    "# 3. Using isin() for multiple values\n",
    "selected_products = sales_df[sales_df['Product'].isin(['Laptop', 'Phone'])]\n",
    "print(f\"\\nLaptop and Phone sales: {len(selected_products)} records\")\n",
    "print(selected_products['Product'].value_counts())\n",
    "\n",
    "# 4. String operations for filtering\n",
    "employees_starting_with_A = employees_df[employees_df['Name'].str.startswith('A')]\n",
    "print(f\"\\nEmployees whose names start with 'A': {len(employees_starting_with_A)}\")\n",
    "display(employees_starting_with_A[['Name', 'Department', 'Salary']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c758764",
   "metadata": {},
   "source": [
    "## üßÆ Section 2: Advanced Filtering with .query() Method\n",
    "\n",
    "The `.query()` method provides a more readable way to filter data, especially for complex conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b01de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using .query() method for more readable filtering\n",
    "print(\"üßÆ USING .query() METHOD\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# 1. Simple query\n",
    "high_performers = employees_df.query('Performance_Score > 8.5')\n",
    "print(f\"High performers (score > 8.5): {len(high_performers)}\")\n",
    "display(high_performers[['Name', 'Department', 'Performance_Score', 'Salary']])\n",
    "\n",
    "# 2. Complex conditions\n",
    "senior_high_earners = employees_df.query('Age > 35 and Salary > 70000')\n",
    "print(f\"\\nSenior high earners: {len(senior_high_earners)}\")\n",
    "display(senior_high_earners[['Name', 'Age', 'Salary', 'Department']])\n",
    "\n",
    "# 3. Using variables in queries\n",
    "min_salary = 70000\n",
    "target_dept = 'Engineering'\n",
    "result = employees_df.query('Salary >= @min_salary and Department == @target_dept')\n",
    "print(f\"\\nEngineering employees with salary >= ${min_salary}: {len(result)}\")\n",
    "display(result[['Name', 'Salary', 'Performance_Score']])\n",
    "\n",
    "# 4. Date filtering with query\n",
    "recent_sales = sales_df.query(\"Date >= '2024-01-20'\")\n",
    "print(f\"\\nSales from Jan 20 onwards: {len(recent_sales)}\")\n",
    "print(f\"Total revenue in this period: ${recent_sales['Revenue'].sum():,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e52bc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and load data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Set pandas options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "# Load and prepare data\n",
    "sales_df = pd.read_csv('../data/sales_data.csv')\n",
    "employees_df = pd.read_csv('../data/employees.csv')\n",
    "weather_df = pd.read_csv('../data/weather_data.csv')\n",
    "\n",
    "# Convert date columns\n",
    "sales_df['Date'] = pd.to_datetime(sales_df['Date'])\n",
    "employees_df['Join_Date'] = pd.to_datetime(employees_df['Join_Date'])\n",
    "weather_df['Date'] = pd.to_datetime(weather_df['Date'])\n",
    "\n",
    "# Add some intentional \"messy\" data for cleaning examples\n",
    "np.random.seed(42)\n",
    "\n",
    "# Add some missing values\n",
    "sales_df.loc[np.random.choice(sales_df.index, 3), 'Revenue'] = np.nan\n",
    "employees_df.loc[np.random.choice(employees_df.index, 2), 'Performance_Score'] = np.nan\n",
    "\n",
    "# Add some outliers\n",
    "sales_df.loc[sales_df.index[-1], 'Revenue'] = 50000  # Extreme outlier\n",
    "\n",
    "print(\"‚úÖ Data loaded and 'messy' data added for cleaning practice!\")\n",
    "print(f\"üìä Sales data shape: {sales_df.shape}\")\n",
    "print(f\"üë• Employee data shape: {employees_df.shape}\")\n",
    "print(f\"üå§Ô∏è Weather data shape: {weather_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66334d6",
   "metadata": {},
   "source": [
    "## üîç Section 1: Boolean Indexing and Filtering\n",
    "\n",
    "Boolean indexing is one of the most powerful features in pandas for selecting specific subsets of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97350af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean indexing examples\n",
    "print(\"üéØ BOOLEAN INDEXING AND FILTERING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Simple condition filtering\n",
    "print(\"1Ô∏è‚É£ High Revenue Sales (>$1000):\")\n",
    "high_revenue = sales_df[sales_df['Revenue'] > 1000]\n",
    "print(f\"Found {len(high_revenue)} high revenue transactions\")\n",
    "display(high_revenue.head())\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# 2. Multiple conditions with & (and) operator\n",
    "print(\"2Ô∏è‚É£ Electronics sales in North region:\")\n",
    "electronics_north = sales_df[\n",
    "    (sales_df['Category'] == 'Electronics') & \n",
    "    (sales_df['Region'] == 'North')\n",
    "]\n",
    "print(f\"Found {len(electronics_north)} electronics sales in North\")\n",
    "display(electronics_north)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# 3. Using | (or) operator\n",
    "print(\"3Ô∏è‚É£ High salary OR high performance employees:\")\n",
    "high_performers = employees_df[\n",
    "    (employees_df['Salary'] > 75000) | \n",
    "    (employees_df['Performance_Score'] > 8.5)\n",
    "]\n",
    "print(f\"Found {len(high_performers)} high-performing employees\")\n",
    "display(high_performers[['Name', 'Department', 'Salary', 'Performance_Score']])\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# 4. String filtering\n",
    "print(\"4Ô∏è‚É£ Employees with names starting with 'A':\")\n",
    "a_names = employees_df[employees_df['Name'].str.startswith('A')]\n",
    "display(a_names[['Name', 'Department', 'Age']])\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# 5. Date-based filtering\n",
    "print(\"5Ô∏è‚É£ Sales from the first week of January:\")\n",
    "first_week = sales_df[sales_df['Date'] <= '2024-01-07']\n",
    "print(f\"Found {len(first_week)} sales in the first week\")\n",
    "display(first_week[['Date', 'Product', 'Revenue']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a46b7d",
   "metadata": {},
   "source": [
    "## üßπ Section 2: Handling Missing Data\n",
    "\n",
    "Missing data is a common challenge in real-world datasets. Let's explore various strategies to handle it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32adedd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing data analysis and handling\n",
    "print(\"üö® MISSING DATA ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check for missing values\n",
    "print(\"1Ô∏è‚É£ Missing Values Summary:\")\n",
    "for name, df in [('Sales', sales_df), ('Employees', employees_df), ('Weather', weather_df)]:\n",
    "    missing = df.isnull().sum()\n",
    "    if missing.sum() > 0:\n",
    "        print(f\"\\n{name} Dataset Missing Values:\")\n",
    "        for col, count in missing[missing > 0].items():\n",
    "            pct = (count / len(df)) * 100\n",
    "            print(f\"  {col}: {count} ({pct:.1f}%)\")\n",
    "    else:\n",
    "        print(f\"\\n{name} Dataset: No missing values\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Different strategies for handling missing values\n",
    "print(\"2Ô∏è‚É£ Missing Data Handling Strategies:\")\n",
    "\n",
    "# Strategy 1: Remove rows with missing values\n",
    "print(\"\\na) Remove rows with missing Revenue:\")\n",
    "sales_clean_drop = sales_df.dropna(subset=['Revenue'])\n",
    "print(f\"Original: {len(sales_df)} rows ‚Üí After dropping: {len(sales_clean_drop)} rows\")\n",
    "\n",
    "# Strategy 2: Fill with mean\n",
    "print(\"\\nb) Fill missing Revenue with mean:\")\n",
    "sales_clean_mean = sales_df.copy()\n",
    "revenue_mean = sales_df['Revenue'].mean()\n",
    "sales_clean_mean['Revenue'] = sales_clean_mean['Revenue'].fillna(revenue_mean)\n",
    "print(f\"Filled {sales_df['Revenue'].isnull().sum()} missing values with mean: ${revenue_mean:.2f}\")\n",
    "\n",
    "# Strategy 3: Fill with median (more robust to outliers)\n",
    "print(\"\\nc) Fill missing Performance Scores with median:\")\n",
    "employees_clean = employees_df.copy()\n",
    "perf_median = employees_df['Performance_Score'].median()\n",
    "employees_clean['Performance_Score'] = employees_clean['Performance_Score'].fillna(perf_median)\n",
    "print(f\"Filled {employees_df['Performance_Score'].isnull().sum()} missing values with median: {perf_median}\")\n",
    "\n",
    "# Strategy 4: Forward/backward fill\n",
    "print(\"\\nd) Forward fill for time series data:\")\n",
    "weather_clean = weather_df.copy()\n",
    "print(\"Before forward fill:\")\n",
    "print(weather_clean.isnull().sum())\n",
    "weather_clean = weather_clean.fillna(method='ffill')\n",
    "print(\"After forward fill:\")\n",
    "print(weather_clean.isnull().sum())\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Interpolation for numerical data\n",
    "print(\"3Ô∏è‚É£ Interpolation for Missing Values:\")\n",
    "# Create a series with some missing values for demonstration\n",
    "demo_series = pd.Series([1, 2, np.nan, 4, np.nan, 6, 7])\n",
    "print(\"Original series:\", demo_series.tolist())\n",
    "print(\"Linear interpolation:\", demo_series.interpolate().tolist())\n",
    "print(\"Polynomial interpolation:\", demo_series.interpolate(method='polynomial', order=2).round(2).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df505c6",
   "metadata": {},
   "source": [
    "## üéØ Section 3: Outlier Detection and Treatment\n",
    "\n",
    "Outliers can significantly impact your analysis. Let's learn how to identify and handle them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6498ef49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier detection and treatment\n",
    "print(\"üéØ OUTLIER DETECTION AND TREATMENT\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Statistical outlier detection using IQR\n",
    "print(\"1Ô∏è‚É£ IQR Method for Outlier Detection:\")\n",
    "def detect_outliers_iqr(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
    "    return outliers, lower_bound, upper_bound\n",
    "\n",
    "# Check for outliers in Revenue\n",
    "revenue_outliers, lower, upper = detect_outliers_iqr(sales_df, 'Revenue')\n",
    "print(f\"Revenue outliers (outside ${lower:.2f} - ${upper:.2f}):\")\n",
    "display(revenue_outliers[['Date', 'Product', 'Revenue']])\n",
    "\n",
    "print(f\"\\nFound {len(revenue_outliers)} outliers out of {len(sales_df)} records\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# 2. Z-score method\n",
    "print(\"2Ô∏è‚É£ Z-Score Method for Outlier Detection:\")\n",
    "from scipy import stats\n",
    "\n",
    "def detect_outliers_zscore(df, column, threshold=3):\n",
    "    z_scores = np.abs(stats.zscore(df[column].dropna()))\n",
    "    outlier_indices = df.dropna()[z_scores > threshold].index\n",
    "    return df.loc[outlier_indices]\n",
    "\n",
    "# Apply z-score method\n",
    "zscore_outliers = detect_outliers_zscore(sales_df, 'Revenue')\n",
    "print(f\"Z-score outliers (|z| > 3):\")\n",
    "display(zscore_outliers[['Date', 'Product', 'Revenue']])\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# 3. Visualizing outliers\n",
    "print(\"3Ô∏è‚É£ Visualizing Outliers:\")\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Box plot\n",
    "sales_df['Revenue'].plot(kind='box', ax=axes[0])\n",
    "axes[0].set_title('üìä Revenue Box Plot\\n(Shows outliers as points)')\n",
    "axes[0].set_ylabel('Revenue ($)')\n",
    "\n",
    "# Histogram\n",
    "sales_df['Revenue'].hist(bins=15, ax=axes[1], alpha=0.7, color='skyblue')\n",
    "axes[1].set_title('üìà Revenue Distribution')\n",
    "axes[1].set_xlabel('Revenue ($)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].axvline(upper, color='red', linestyle='--', label=f'Upper Threshold: ${upper:.0f}')\n",
    "axes[1].legend()\n",
    "\n",
    "# Scatter plot with outliers highlighted\n",
    "sales_df.reset_index().plot.scatter(x='index', y='Revenue', ax=axes[2], alpha=0.6)\n",
    "outlier_indices = revenue_outliers.index\n",
    "axes[2].scatter(outlier_indices, sales_df.loc[outlier_indices, 'Revenue'], \n",
    "               color='red', s=100, alpha=0.8, label='Outliers')\n",
    "axes[2].set_title('üî¥ Revenue Over Time\\n(Outliers in Red)')\n",
    "axes[2].set_xlabel('Transaction Index')\n",
    "axes[2].set_ylabel('Revenue ($)')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# 4. Outlier treatment strategies\n",
    "print(\"4Ô∏è‚É£ Outlier Treatment Strategies:\")\n",
    "\n",
    "# Strategy A: Remove outliers\n",
    "sales_no_outliers = sales_df[~sales_df.index.isin(revenue_outliers.index)]\n",
    "print(f\"a) Remove outliers: {len(sales_df)} ‚Üí {len(sales_no_outliers)} records\")\n",
    "\n",
    "# Strategy B: Cap outliers (Winsorization)\n",
    "sales_capped = sales_df.copy()\n",
    "sales_capped.loc[sales_capped['Revenue'] > upper, 'Revenue'] = upper\n",
    "sales_capped.loc[sales_capped['Revenue'] < lower, 'Revenue'] = lower\n",
    "print(f\"b) Cap outliers: Max revenue before: ${sales_df['Revenue'].max():.2f}, after: ${sales_capped['Revenue'].max():.2f}\")\n",
    "\n",
    "# Strategy C: Transform data (log transformation)\n",
    "sales_log = sales_df.copy()\n",
    "sales_log['Revenue_log'] = np.log1p(sales_log['Revenue'])  # log1p handles zeros better\n",
    "print(f\"c) Log transformation: Revenue range {sales_df['Revenue'].min():.2f}-{sales_df['Revenue'].max():.2f} ‚Üí {sales_log['Revenue_log'].min():.2f}-{sales_log['Revenue_log'].max():.2f}\")\n",
    "\n",
    "# Compare distributions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "sales_df['Revenue'].hist(bins=15, ax=axes[0], alpha=0.7, color='lightblue')\n",
    "axes[0].set_title('Original Revenue Distribution')\n",
    "axes[0].set_xlabel('Revenue ($)')\n",
    "\n",
    "sales_log['Revenue_log'].hist(bins=15, ax=axes[1], alpha=0.7, color='lightgreen')\n",
    "axes[1].set_title('Log-Transformed Revenue Distribution')\n",
    "axes[1].set_xlabel('Log(Revenue)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bfbbda",
   "metadata": {},
   "source": [
    "## ‚è∞ Section 4: Time Series Analysis and Date Operations\n",
    "\n",
    "Working with dates and time series data is crucial for many real-world analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f778cc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series analysis and date operations\n",
    "print(\"‚è∞ TIME SERIES ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Extract date components\n",
    "print(\"1Ô∏è‚É£ Extracting Date Components:\")\n",
    "sales_df['Year'] = sales_df['Date'].dt.year\n",
    "sales_df['Month'] = sales_df['Date'].dt.month\n",
    "sales_df['DayOfWeek'] = sales_df['Date'].dt.dayofweek\n",
    "sales_df['DayName'] = sales_df['Date'].dt.day_name()\n",
    "sales_df['IsWeekend'] = sales_df['DayOfWeek'].isin([5, 6])\n",
    "\n",
    "print(\"Sample with date components:\")\n",
    "display(sales_df[['Date', 'Year', 'Month', 'DayName', 'IsWeekend', 'Revenue']].head())\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# 2. Time-based filtering\n",
    "print(\"2Ô∏è‚É£ Time-Based Filtering:\")\n",
    "\n",
    "# Last week of January\n",
    "last_week = sales_df[sales_df['Date'] >= '2024-01-24']\n",
    "print(f\"Sales in last week of January: {len(last_week)} transactions\")\n",
    "\n",
    "# Weekend vs Weekday analysis\n",
    "weekend_sales = sales_df[sales_df['IsWeekend'] == True]['Revenue'].sum()\n",
    "weekday_sales = sales_df[sales_df['IsWeekend'] == False]['Revenue'].sum()\n",
    "print(f\"Weekend sales: ${weekend_sales:,.2f}\")\n",
    "print(f\"Weekday sales: ${weekday_sales:,.2f}\")\n",
    "print(f\"Weekend sales ratio: {weekend_sales/(weekend_sales + weekday_sales)*100:.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# 3. Time series resampling and aggregation\n",
    "print(\"3Ô∏è‚É£ Time Series Resampling:\")\n",
    "\n",
    "# Set date as index for resampling\n",
    "sales_ts = sales_df.set_index('Date')\n",
    "\n",
    "# Daily aggregation (already daily, but shows the concept)\n",
    "daily_sales = sales_ts.resample('D')['Revenue'].sum()\n",
    "print(\"Daily sales summary:\")\n",
    "print(daily_sales.head())\n",
    "\n",
    "# Weekly aggregation\n",
    "weekly_sales = sales_ts.resample('W')['Revenue'].agg(['sum', 'mean', 'count'])\n",
    "print(\"\\nWeekly sales summary:\")\n",
    "display(weekly_sales)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# 4. Rolling window calculations\n",
    "print(\"4Ô∏è‚É£ Rolling Window Calculations:\")\n",
    "\n",
    "# 3-day rolling average\n",
    "sales_ts_clean = sales_ts.dropna(subset=['Revenue'])  # Remove NaN for rolling calculation\n",
    "sales_ts_clean['Revenue_3day_avg'] = sales_ts_clean['Revenue'].rolling(window=3).mean()\n",
    "sales_ts_clean['Revenue_3day_sum'] = sales_ts_clean['Revenue'].rolling(window=3).sum()\n",
    "\n",
    "print(\"Revenue with rolling calculations:\")\n",
    "display(sales_ts_clean[['Revenue', 'Revenue_3day_avg', 'Revenue_3day_sum']].head(10))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# 5. Date difference calculations\n",
    "print(\"5Ô∏è‚É£ Date Difference Calculations:\")\n",
    "\n",
    "# Calculate days since each employee joined\n",
    "employees_df['Days_Since_Joining'] = (datetime.now() - employees_df['Join_Date']).dt.days\n",
    "employees_df['Years_Experience_Calculated'] = employees_df['Days_Since_Joining'] / 365.25\n",
    "\n",
    "print(\"Employee tenure analysis:\")\n",
    "display(employees_df[['Name', 'Join_Date', 'Days_Since_Joining', 'Years_Experience_Calculated']].head())\n",
    "\n",
    "# Compare with provided experience\n",
    "tenure_comparison = employees_df[['Name', 'Experience_Years', 'Years_Experience_Calculated']].head()\n",
    "tenure_comparison['Difference'] = (tenure_comparison['Years_Experience_Calculated'] - \n",
    "                                 tenure_comparison['Experience_Years']).round(1)\n",
    "print(\"\\nExperience comparison (calculated vs provided):\")\n",
    "display(tenure_comparison)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# 6. Time series visualization\n",
    "print(\"6Ô∏è‚É£ Time Series Visualization:\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Daily revenue trend\n",
    "daily_sales.plot(ax=axes[0,0], marker='o', linewidth=2, markersize=4)\n",
    "axes[0,0].set_title('üìà Daily Revenue Trend')\n",
    "axes[0,0].set_ylabel('Revenue ($)')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Revenue by day of week\n",
    "day_revenue = sales_df.groupby('DayName')['Revenue'].mean()\n",
    "day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "day_revenue = day_revenue.reindex(day_order)\n",
    "day_revenue.plot(kind='bar', ax=axes[0,1], color='lightgreen', alpha=0.8)\n",
    "axes[0,1].set_title('üìä Average Revenue by Day of Week')\n",
    "axes[0,1].set_ylabel('Average Revenue ($)')\n",
    "axes[0,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Rolling average\n",
    "sales_ts_clean[['Revenue', 'Revenue_3day_avg']].plot(ax=axes[1,0], marker='o', markersize=3)\n",
    "axes[1,0].set_title('üîÑ Revenue with 3-Day Rolling Average')\n",
    "axes[1,0].set_ylabel('Revenue ($)')\n",
    "axes[1,0].legend(['Daily Revenue', '3-Day Average'])\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Weekend vs Weekday box plot\n",
    "sales_df.boxplot(column='Revenue', by='IsWeekend', ax=axes[1,1])\n",
    "axes[1,1].set_title('üí∞ Revenue Distribution: Weekday vs Weekend')\n",
    "axes[1,1].set_xlabel('Is Weekend')\n",
    "axes[1,1].set_ylabel('Revenue ($)')\n",
    "axes[1,1].set_xticklabels(['Weekday', 'Weekend'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab4a341",
   "metadata": {},
   "source": [
    "## üéì Section 5: Advanced Cleaning Techniques and Best Practices\n",
    "\n",
    "Let's wrap up with some advanced techniques and professional data cleaning practices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c4b36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced cleaning techniques\n",
    "print(\"üèÜ ADVANCED CLEANING TECHNIQUES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Data validation and quality checks\n",
    "print(\"1Ô∏è‚É£ Data Validation and Quality Checks:\")\n",
    "\n",
    "def data_quality_report(df, name):\n",
    "    \"\"\"Generate a comprehensive data quality report\"\"\"\n",
    "    print(f\"\\nüìä Data Quality Report for {name}:\")\n",
    "    print(f\"  Shape: {df.shape}\")\n",
    "    print(f\"  Missing values: {df.isnull().sum().sum()}\")\n",
    "    print(f\"  Duplicate rows: {df.duplicated().sum()}\")\n",
    "    print(f\"  Memory usage: {df.memory_usage(deep=True).sum() / 1024:.1f} KB\")\n",
    "    \n",
    "    # Check for mixed data types in object columns\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        unique_types = df[col].dropna().apply(type).unique()\n",
    "        if len(unique_types) > 1:\n",
    "            print(f\"  ‚ö†Ô∏è Mixed types in {col}: {[t.__name__ for t in unique_types]}\")\n",
    "    \n",
    "    # Check for potential outliers in numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        outliers = df[(df[col] < Q1 - 1.5*IQR) | (df[col] > Q3 + 1.5*IQR)]\n",
    "        if len(outliers) > 0:\n",
    "            print(f\"  ‚ö†Ô∏è Potential outliers in {col}: {len(outliers)} ({len(outliers)/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Apply quality checks\n",
    "for name, df in [('Sales', sales_df), ('Employees', employees_df), ('Weather', weather_df)]:\n",
    "    data_quality_report(df, name)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# 2. String cleaning and standardization\n",
    "print(\"2Ô∏è‚É£ String Cleaning and Standardization:\")\n",
    "\n",
    "# Demonstrate with product names (add some messy data)\n",
    "messy_products = sales_df['Product'].copy()\n",
    "messy_products.iloc[0] = '  LAPTOP  '  # Extra spaces\n",
    "messy_products.iloc[1] = 'phone'       # Inconsistent case\n",
    "messy_products.iloc[2] = 'Desk-Chair'  # Different separator\n",
    "\n",
    "print(\"Before cleaning:\")\n",
    "print(messy_products.head().tolist())\n",
    "\n",
    "# Clean the strings\n",
    "clean_products = (messy_products\n",
    "                 .str.strip()           # Remove leading/trailing spaces\n",
    "                 .str.title()           # Standardize case\n",
    "                 .str.replace('-', ' ') # Standardize separators\n",
    "                 .str.replace(r'\\s+', ' ', regex=True))  # Remove extra spaces\n",
    "\n",
    "print(\"After cleaning:\")\n",
    "print(clean_products.head().tolist())\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# 3. Data type optimization\n",
    "print(\"3Ô∏è‚É£ Data Type Optimization:\")\n",
    "\n",
    "def optimize_datatypes(df):\n",
    "    \"\"\"Optimize data types to reduce memory usage\"\"\"\n",
    "    df_opt = df.copy()\n",
    "    \n",
    "    # Convert integers to smaller types where possible\n",
    "    for col in df_opt.select_dtypes(include=['int64']).columns:\n",
    "        col_min = df_opt[col].min()\n",
    "        col_max = df_opt[col].max()\n",
    "        \n",
    "        if col_min >= 0:  # Unsigned integers\n",
    "            if col_max < 255:\n",
    "                df_opt[col] = df_opt[col].astype('uint8')\n",
    "            elif col_max < 65535:\n",
    "                df_opt[col] = df_opt[col].astype('uint16')\n",
    "            elif col_max < 4294967295:\n",
    "                df_opt[col] = df_opt[col].astype('uint32')\n",
    "        else:  # Signed integers\n",
    "            if col_min > -128 and col_max < 127:\n",
    "                df_opt[col] = df_opt[col].astype('int8')\n",
    "            elif col_min > -32768 and col_max < 32767:\n",
    "                df_opt[col] = df_opt[col].astype('int16')\n",
    "            elif col_min > -2147483648 and col_max < 2147483647:\n",
    "                df_opt[col] = df_opt[col].astype('int32')\n",
    "    \n",
    "    # Convert float64 to float32 where precision allows\n",
    "    for col in df_opt.select_dtypes(include=['float64']).columns:\n",
    "        df_opt[col] = pd.to_numeric(df_opt[col], downcast='float')\n",
    "    \n",
    "    # Convert object columns to category where appropriate\n",
    "    for col in df_opt.select_dtypes(include=['object']).columns:\n",
    "        if df_opt[col].nunique() / len(df_opt) < 0.5:  # Less than 50% unique values\n",
    "            df_opt[col] = df_opt[col].astype('category')\n",
    "    \n",
    "    return df_opt\n",
    "\n",
    "# Apply optimization\n",
    "sales_optimized = optimize_datatypes(sales_df)\n",
    "\n",
    "print(\"Memory usage comparison:\")\n",
    "print(f\"Original: {sales_df.memory_usage(deep=True).sum() / 1024:.1f} KB\")\n",
    "print(f\"Optimized: {sales_optimized.memory_usage(deep=True).sum() / 1024:.1f} KB\")\n",
    "print(f\"Reduction: {(1 - sales_optimized.memory_usage(deep=True).sum() / sales_df.memory_usage(deep=True).sum()) * 100:.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# 4. Create a comprehensive cleaning pipeline\n",
    "print(\"4Ô∏è‚É£ Comprehensive Cleaning Pipeline:\")\n",
    "\n",
    "def clean_dataset(df, name):\n",
    "    \"\"\"Comprehensive data cleaning pipeline\"\"\"\n",
    "    print(f\"\\nüßπ Cleaning {name} dataset...\")\n",
    "    \n",
    "    # Store original shape\n",
    "    original_shape = df.shape\n",
    "    \n",
    "    # Remove duplicates\n",
    "    df_clean = df.drop_duplicates()\n",
    "    if df_clean.shape[0] != original_shape[0]:\n",
    "        print(f\"  Removed {original_shape[0] - df_clean.shape[0]} duplicate rows\")\n",
    "    \n",
    "    # Handle missing values (strategy depends on column type and business logic)\n",
    "    numeric_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        if df_clean[col].isnull().sum() > 0:\n",
    "            # Fill with median for robustness against outliers\n",
    "            df_clean[col] = df_clean[col].fillna(df_clean[col].median())\n",
    "            print(f\"  Filled missing values in {col} with median\")\n",
    "    \n",
    "    # Clean string columns\n",
    "    string_cols = df_clean.select_dtypes(include=['object']).columns\n",
    "    for col in string_cols:\n",
    "        if col not in ['Date']:  # Skip date columns\n",
    "            df_clean[col] = (df_clean[col]\n",
    "                           .astype(str)\n",
    "                           .str.strip()\n",
    "                           .str.title()\n",
    "                           .replace('Nan', np.nan))\n",
    "    \n",
    "    # Optimize data types\n",
    "    df_clean = optimize_datatypes(df_clean)\n",
    "    \n",
    "    print(f\"  Final shape: {df_clean.shape} (removed {original_shape[0] - df_clean.shape[0]} rows)\")\n",
    "    return df_clean\n",
    "\n",
    "# Apply cleaning pipeline\n",
    "sales_clean = clean_dataset(sales_df, \"Sales\")\n",
    "employees_clean = clean_dataset(employees_df, \"Employees\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# 5. Final summary and recommendations\n",
    "print(\"5Ô∏è‚É£ Data Cleaning Best Practices Summary:\")\n",
    "print(\"\"\"\n",
    "‚úÖ BEST PRACTICES CHECKLIST:\n",
    "\n",
    "üìä Assessment:\n",
    "  ‚Ä¢ Always start with exploratory data analysis\n",
    "  ‚Ä¢ Check data types, shapes, and basic statistics\n",
    "  ‚Ä¢ Identify missing values, duplicates, and outliers\n",
    "\n",
    "üßπ Cleaning Strategy:\n",
    "  ‚Ä¢ Document all cleaning decisions and rationale\n",
    "  ‚Ä¢ Keep original data intact (work on copies)\n",
    "  ‚Ä¢ Apply cleaning consistently across datasets\n",
    "  ‚Ä¢ Validate results after each cleaning step\n",
    "\n",
    "üéØ Quality Assurance:\n",
    "  ‚Ä¢ Set up automated data quality checks\n",
    "  ‚Ä¢ Monitor data quality over time\n",
    "  ‚Ä¢ Use appropriate imputation methods for missing data\n",
    "  ‚Ä¢ Handle outliers based on domain knowledge\n",
    "\n",
    "‚ö° Performance:\n",
    "  ‚Ä¢ Optimize data types for memory efficiency\n",
    "  ‚Ä¢ Use vectorized operations when possible\n",
    "  ‚Ä¢ Consider chunk processing for large datasets\n",
    "  ‚Ä¢ Profile memory usage regularly\n",
    "\n",
    "üìù Documentation:\n",
    "  ‚Ä¢ Document cleaning steps and assumptions\n",
    "  ‚Ä¢ Create reusable cleaning functions\n",
    "  ‚Ä¢ Version control your cleaning pipelines\n",
    "  ‚Ä¢ Share data dictionaries with your team\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
